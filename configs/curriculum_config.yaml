# Curriculum Learning Configuration

meta_token:
  # Distance metric for meta token difference
  distance_metric: "l2"  # Options: l2, cosine, kl
  
  # Top-K layers for difficulty calculation
  top_k: 3
  
  # System prompt for meta token generation
  system_prompt: "Analyze the following dialogue and generate embeddings for task understanding."
  
  # Token pooling method
  pooling: "mean"  # Options: mean, cls, last

curriculum:
  # Curriculum strategy
  strategy: "layer_wise_progressive"  # Options: easy_to_hard, layer_wise_progressive, dynamic_pacing, hybrid
  
  # Number of curriculum stages
  num_stages: 3
  
  # Stage percentiles (if null, will be evenly divided)
  stage_percentiles: [0.33, 0.66, 1.0]
  
  # Primary difficulty metric for sorting
  difficulty_metric: "topk_mean"  # Options: mean, max, topk_mean, weighted_mean, median
  
  # Dynamic pacing parameters
  loss_threshold: 2.0
  difficulty_step_up: 0.05
  difficulty_step_down: 0.02
  
  # Training schedule
  curriculum_start_epoch: 0  # Start curriculum from this epoch
  curriculum_end_epoch: 5    # End curriculum at this epoch (use all data after)
  warmup_epochs: 1           # First N epochs use only easiest samples

training:
  # Model settings
  cloud_model: "meta-llama/Meta-Llama-3-8B"
  edge_model: "Qwen/Qwen2.5-1.5B"
  
  # Training hyperparameters
  batch_size: 4
  learning_rate: 1e-4
  num_epochs: 10
  gradient_accumulation_steps: 4
  
  # LoRA settings
  lora_rank: 16
  lora_alpha: 32
  
  # Device
  device: "cuda"

data:
  # Training data path
  train_data_path: "data/train.jsonl"
  
  # Validation data path
  val_data_path: "data/val.jsonl"
  
  # Difficulty labels path (generated by 01_label_difficulties.py)
  difficulty_labels_path: "curriculum/data/difficulty_labels.json"
  
  # Max sequence length
  max_length: 512

output:
  # Checkpoint directory
  checkpoint_dir: "outputs/curriculum_checkpoints"
  
  # Log directory
  log_dir: "outputs/curriculum_logs"
  
  # Save frequency (epochs)
  save_every: 1

evaluation:
  # Evaluate every N epochs
  eval_every: 1
  
  # Evaluation metrics
  metrics: ["loss", "perplexity", "accuracy"]

experiment:
  # Experiment name
  name: "curriculum_learning_baseline"
  
  # Random seed
  seed: 42
  
  # Enable wandb logging
  use_wandb: false
  wandb_project: "lora-gen-curriculum"

"""
ë©”íƒ€í† í° ì¶”ì¶œ ë° ì°¨ì´ ê³„ì‚° ëª¨ë“ˆ

Cloud LLMê³¼ Edge LLMì—ì„œ ë ˆì´ì–´ë³„ ë©”íƒ€í† í°ì„ ì¶”ì¶œí•˜ê³ ,
ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ìƒ˜í”Œì˜ ë‚œì´ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional
from transformers import AutoModel, AutoTokenizer
import numpy as np
from dataclasses import dataclass
import json


@dataclass
class MetaTokenDifference:
    """ë©”íƒ€í† í° ì°¨ì´ ì •ë³´"""
    sample_id: str
    layer_diffs: Dict[int, float]  # {layer_idx: difference}
    difficulty_scores: Dict[str, float]  # {metric_name: score}
    difficulty_percentile: float
    curriculum_stage: int


class MetaTokenExtractor:
    """
    Cloud/Edge LLMì—ì„œ ë ˆì´ì–´ë³„ ë©”íƒ€í† í° ì¶”ì¶œ
    """
    
    def __init__(
        self,
        cloud_model_name: str = "meta-llama/Meta-Llama-3-8B",
        edge_model_name: str = "Qwen/Qwen2.5-1.5B",
        system_prompt: str = "Analyze the following dialogue and generate embeddings for task understanding.",
        device: str = "cuda",
        use_fp16: bool = True,
    ):
        self.cloud_model_name = cloud_model_name
        self.edge_model_name = edge_model_name
        self.system_prompt = system_prompt
        self.device = device
        self.use_fp16 = use_fp16
        
        print(f"Loading Cloud LLM: {cloud_model_name}")
        self.cloud_tokenizer = AutoTokenizer.from_pretrained(cloud_model_name)
        if self.cloud_tokenizer.pad_token is None:
            self.cloud_tokenizer.pad_token = self.cloud_tokenizer.eos_token
        self.cloud_model = AutoModel.from_pretrained(
            cloud_model_name,
            torch_dtype=torch.float16 if use_fp16 else torch.float32,
            device_map=device,
            trust_remote_code=True,
        )
        self.cloud_model.eval()
        for param in self.cloud_model.parameters():
            param.requires_grad = False
        print(f"  âœ“ Cloud LLM loaded ({self.cloud_model.config.num_hidden_layers} layers)")
        
        print(f"Loading Edge LLM: {edge_model_name}")
        self.edge_tokenizer = AutoTokenizer.from_pretrained(edge_model_name)
        if self.edge_tokenizer.pad_token is None:
            self.edge_tokenizer.pad_token = self.edge_tokenizer.eos_token
        self.edge_model = AutoModel.from_pretrained(
            edge_model_name,
            torch_dtype=torch.float16 if use_fp16 else torch.float32,
            device_map=device,
            trust_remote_code=True,
        )
        self.edge_model.eval()
        for param in self.edge_model.parameters():
            param.requires_grad = False
        print(f"  âœ“ Edge LLM loaded ({self.edge_model.config.num_hidden_layers} layers)")
        
        self.cloud_num_layers = self.cloud_model.config.num_hidden_layers
        self.edge_num_layers = self.edge_model.config.num_hidden_layers
        
        # ë ˆì´ì–´ ë§¤ì¹­ ì „ëµ: Edge ë ˆì´ì–´ë¥¼ Cloud ë ˆì´ì–´ì— ë§¤í•‘
        # EdgeëŠ” ë ˆì´ì–´ê°€ ì ìœ¼ë¯€ë¡œ (ì˜ˆ: 28ì¸µ), Cloud (ì˜ˆ: 32ì¸µ)ì— ë¹„ë¡€ ë§¤í•‘
        self.layer_mapping = self._create_layer_mapping()
        
    def _create_layer_mapping(self) -> Dict[int, int]:
        """
        Edge ë ˆì´ì–´ë¥¼ Cloud ë ˆì´ì–´ì— ë§¤í•‘
        
        ì˜ˆ: Edge 28ì¸µ â†’ Cloud 32ì¸µ
        Edge layer 0 â†’ Cloud layer 0
        Edge layer 14 â†’ Cloud layer 16
        Edge layer 27 â†’ Cloud layer 31
        """
        mapping = {}
        for edge_idx in range(self.edge_num_layers):
            cloud_idx = int(edge_idx * self.cloud_num_layers / self.edge_num_layers)
            mapping[edge_idx] = cloud_idx
        return mapping
    
    def _format_input(self, dialogue_history: Optional[List[str]] = None, current_utterance: Optional[str] = None, full_text: Optional[str] = None) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ í˜„ì¬ ë°œí™”ë¥¼ í¬ë§·íŒ… (ëŒ€í™” í˜•ì‹ ë˜ëŠ” reasoning task í˜•ì‹)"""
        if full_text is not None:
            # Reasoning task format: use full_text directly
            return self.system_prompt + "\n\n" + full_text
        else:
            # Dialogue format: combine history + utterance
            formatted = self.system_prompt + "\n\n"
            for i, turn in enumerate(dialogue_history):
                formatted += f"Turn {i+1}: {turn}\n"
            formatted += f"Current: {current_utterance}"
            return formatted
    
    @torch.no_grad()
    def extract_meta_tokens_batch(
        self,
        texts: List[str],
        pooling: str = "mean",
    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """
        ë°°ì¹˜ë¡œ ë©”íƒ€í† í° ì¶”ì¶œ (ì†ë„ ìµœì í™”)
        
        Args:
            texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            pooling: í† í° í’€ë§ ë°©ë²•
            
        Returns:
            cloud_meta_tokens_list: List of [num_cloud_layers, hidden_dim]
            edge_meta_tokens_list: List of [num_edge_layers, hidden_dim]
        """
        # Cloud LLM ë°°ì¹˜ ì¶”ë¡ 
        cloud_inputs = self.cloud_tokenizer(
            texts,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True,
        ).to(self.device)
        
        cloud_outputs = self.cloud_model(
            **cloud_inputs,
            output_hidden_states=True,
        )
        
        # Edge LLM ë°°ì¹˜ ì¶”ë¡ 
        edge_inputs = self.edge_tokenizer(
            texts,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True,
        ).to(self.device)
        
        edge_outputs = self.edge_model(
            **edge_inputs,
            output_hidden_states=True,
        )
        
        # ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œì— ëŒ€í•´ ë©”íƒ€í† í° ì¶”ì¶œ
        batch_size = len(texts)
        cloud_meta_tokens_list = []
        edge_meta_tokens_list = []
        
        for b in range(batch_size):
            # Cloud ë©”íƒ€í† í°
            cloud_tokens = []
            for layer_output in cloud_outputs.hidden_states:
                if pooling == "mean":
                    token = layer_output[b].mean(dim=0)
                elif pooling == "cls":
                    token = layer_output[b, 0, :]
                elif pooling == "last":
                    token = layer_output[b, -1, :]
                cloud_tokens.append(token.cpu())
            cloud_meta_tokens_list.append(torch.stack(cloud_tokens))
            
            # Edge ë©”íƒ€í† í°
            edge_tokens = []
            for layer_output in edge_outputs.hidden_states:
                if pooling == "mean":
                    token = layer_output[b].mean(dim=0)
                elif pooling == "cls":
                    token = layer_output[b, 0, :]
                elif pooling == "last":
                    token = layer_output[b, -1, :]
                edge_tokens.append(token.cpu())
            edge_meta_tokens_list.append(torch.stack(edge_tokens))
        
        return cloud_meta_tokens_list, edge_meta_tokens_list
    
    @torch.no_grad()
    def extract_meta_tokens(
        self,
        dialogue_history: Optional[List[str]] = None,
        current_utterance: Optional[str] = None,
        full_text: Optional[str] = None,
        pooling: str = "mean",  # mean, cls, last
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Cloud/Edge LLMì—ì„œ ë©”íƒ€í† í° ì¶”ì¶œ
        
        Args:
            dialogue_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ëŒ€í™” í˜•ì‹)
            current_utterance: í˜„ì¬ ë°œí™” (ëŒ€í™” í˜•ì‹)
            full_text: ì „ì²´ í…ìŠ¤íŠ¸ (reasoning task í˜•ì‹)
            pooling: í† í° í’€ë§ ë°©ë²• (mean/cls/last)
            
        Returns:
            cloud_meta_tokens: [num_cloud_layers, hidden_dim]
            edge_meta_tokens: [num_edge_layers, hidden_dim]
        """
        # ì…ë ¥ í¬ë§·íŒ…
        input_text = self._format_input(dialogue_history, current_utterance, full_text)
        
        # Cloud LLM ì¶”ë¡ 
        cloud_inputs = self.cloud_tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
        ).to(self.device)
        
        cloud_outputs = self.cloud_model(
            **cloud_inputs,
            output_hidden_states=True,
        )
        
        # Edge LLM ì¶”ë¡ 
        edge_inputs = self.edge_tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
        ).to(self.device)
        
        edge_outputs = self.edge_model(
            **edge_inputs,
            output_hidden_states=True,
        )
        
        # ë©”íƒ€í† í° ì¶”ì¶œ (ê° ë ˆì´ì–´ì—ì„œ)
        cloud_meta_tokens = []
        for layer_output in cloud_outputs.hidden_states:
            if pooling == "mean":
                token = layer_output.mean(dim=1).squeeze(0)  # [hidden_dim]
            elif pooling == "cls":
                token = layer_output[:, 0, :].squeeze(0)
            elif pooling == "last":
                token = layer_output[:, -1, :].squeeze(0)
            cloud_meta_tokens.append(token.cpu())
        
        edge_meta_tokens = []
        for layer_output in edge_outputs.hidden_states:
            if pooling == "mean":
                token = layer_output.mean(dim=1).squeeze(0)
            elif pooling == "cls":
                token = layer_output[:, 0, :].squeeze(0)
            elif pooling == "last":
                token = layer_output[:, -1, :].squeeze(0)
            edge_meta_tokens.append(token.cpu())
        
        cloud_meta_tokens = torch.stack(cloud_meta_tokens)  # [num_layers, dim]
        edge_meta_tokens = torch.stack(edge_meta_tokens)
        
        return cloud_meta_tokens, edge_meta_tokens
    
    def compute_layer_differences(
        self,
        cloud_meta_tokens: torch.Tensor,
        edge_meta_tokens: torch.Tensor,
        metric: str = "l2",  # l2, cosine, kl
    ) -> Dict[int, float]:
        """
        ë ˆì´ì–´ë³„ ë©”íƒ€í† í° ì°¨ì´ ê³„ì‚°
        
        Args:
            cloud_meta_tokens: [num_cloud_layers, dim]
            edge_meta_tokens: [num_edge_layers, dim]
            metric: ì°¨ì´ ë©”íŠ¸ë¦­ (l2/cosine/kl)
            
        Returns:
            layer_diffs: {edge_layer_idx: difference}
        """
        layer_diffs = {}
        
        for edge_idx in range(self.edge_num_layers):
            cloud_idx = self.layer_mapping[edge_idx]
            
            edge_token = edge_meta_tokens[edge_idx]
            cloud_token = cloud_meta_tokens[cloud_idx]
            
            # ì°¨ì› ë§ì¶”ê¸° (í•„ìš” ì‹œ projection)
            if edge_token.shape != cloud_token.shape:
                # ë” ì‘ì€ ì°¨ì›ì— ë§ì¶¤
                min_dim = min(edge_token.shape[0], cloud_token.shape[0])
                edge_token = edge_token[:min_dim]
                cloud_token = cloud_token[:min_dim]
            
            if metric == "l2":
                diff = torch.norm(cloud_token - edge_token, p=2).item()
            elif metric == "cosine":
                cos_sim = F.cosine_similarity(
                    cloud_token.unsqueeze(0),
                    edge_token.unsqueeze(0),
                    dim=1
                ).item()
                diff = 1 - cos_sim  # distance
            elif metric == "kl":
                # Softmax + KL divergence
                cloud_prob = F.softmax(cloud_token, dim=0)
                edge_prob = F.softmax(edge_token, dim=0)
                diff = F.kl_div(
                    edge_prob.log(),
                    cloud_prob,
                    reduction='batchmean'
                ).item()
            
            layer_diffs[edge_idx] = diff
        
        return layer_diffs


class DifficultyScorer:
    """
    ë©”íƒ€í† í° ì°¨ì´ë¥¼ ì¢…í•©í•˜ì—¬ ë‚œì´ë„ ì ìˆ˜ ê³„ì‚°
    """
    
    def __init__(
        self,
        num_layers: int,
        top_k: int = 3,
        layer_weights: Optional[np.ndarray] = None,
        tau: float = 5.0,  # exponential weight decay
    ):
        self.num_layers = num_layers
        self.top_k = top_k
        
        # ë ˆì´ì–´ë³„ ê°€ì¤‘ì¹˜ (ì´ˆê¸° ë ˆì´ì–´ ì¤‘ìš”ë„ ë†’ìŒ)
        if layer_weights is None:
            self.layer_weights = np.exp(-np.arange(num_layers) / tau)
            self.layer_weights /= self.layer_weights.sum()
        else:
            self.layer_weights = layer_weights
    
    def compute_difficulty_scores(
        self,
        layer_diffs: Dict[int, float],
    ) -> Dict[str, float]:
        """
        ì—¬ëŸ¬ ë‚œì´ë„ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Returns:
            {
                'mean': í‰ê·  ì°¨ì´,
                'max': ìµœëŒ€ ì°¨ì´,
                'topk_mean': Top-K í‰ê· ,
                'weighted_mean': ê°€ì¤‘ í‰ê· ,
                'std': í‘œì¤€í¸ì°¨,
            }
        """
        diffs = np.array([layer_diffs[i] for i in range(self.num_layers)])
        
        scores = {
            'mean': float(diffs.mean()),
            'max': float(diffs.max()),
            'topk_mean': float(np.sort(diffs)[-self.top_k:].mean()),
            'weighted_mean': float((diffs * self.layer_weights).sum()),
            'std': float(diffs.std()),
            'median': float(np.median(diffs)),
        }
        
        return scores
    
    def assign_percentile(
        self,
        difficulty_score: float,
        all_scores: List[float],
    ) -> float:
        """
        ì „ì²´ ìƒ˜í”Œ ì¤‘ í˜„ì¬ ìƒ˜í”Œì˜ ë‚œì´ë„ ë°±ë¶„ìœ„ ê³„ì‚°
        
        Returns:
            percentile: 0.0 ~ 1.0
        """
        all_scores_sorted = np.sort(all_scores)
        percentile = np.searchsorted(all_scores_sorted, difficulty_score) / len(all_scores)
        return float(percentile)
    
    def assign_curriculum_stage(
        self,
        percentile: float,
        num_stages: int = 3,
    ) -> int:
        """
        ë°±ë¶„ìœ„ì— ë”°ë¼ ì»¤ë¦¬í˜ëŸ¼ ìŠ¤í…Œì´ì§€ í• ë‹¹
        
        Args:
            percentile: 0.0 ~ 1.0
            num_stages: ìŠ¤í…Œì´ì§€ ìˆ˜
            
        Returns:
            stage: 0 ~ (num_stages-1)
        """
        stage = int(percentile * num_stages)
        return min(stage, num_stages - 1)


class MetaTokenDifferenceAnalyzer:
    """
    ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë©”íƒ€í† í° ì°¨ì´ ë¶„ì„ ë° ë‚œì´ë„ ë ˆì´ë¸”ë§
    """
    
    def __init__(
        self,
        extractor: MetaTokenExtractor,
        scorer: DifficultyScorer,
        metric: str = "l2",
        primary_difficulty_metric: str = "topk_mean",
    ):
        self.extractor = extractor
        self.scorer = scorer
        self.metric = metric
        self.primary_difficulty_metric = primary_difficulty_metric
    
    def analyze_sample(
        self,
        sample_id: str,
        dialogue_history: Optional[List[str]] = None,
        current_utterance: Optional[str] = None,
        full_text: Optional[str] = None,
    ) -> MetaTokenDifference:
        """
        ë‹¨ì¼ ìƒ˜í”Œ ë¶„ì„
        """
        # ë©”íƒ€í† í° ì¶”ì¶œ
        cloud_tokens, edge_tokens = self.extractor.extract_meta_tokens(
            dialogue_history=dialogue_history,
            current_utterance=current_utterance,
            full_text=full_text,
        )
        
        # ë ˆì´ì–´ë³„ ì°¨ì´ ê³„ì‚°
        layer_diffs = self.extractor.compute_layer_differences(
            cloud_tokens,
            edge_tokens,
            metric=self.metric,
        )
        
        # ë‚œì´ë„ ì ìˆ˜ ê³„ì‚°
        difficulty_scores = self.scorer.compute_difficulty_scores(layer_diffs)
        
        # ì„ì‹œ ë°±ë¶„ìœ„ (ë‚˜ì¤‘ì— ì „ì²´ ë°ì´í„°ì…‹ ë³´ê³  ì¬ê³„ì‚°)
        difficulty_percentile = 0.5  # placeholder
        curriculum_stage = 0  # placeholder
        
        return MetaTokenDifference(
            sample_id=sample_id,
            layer_diffs=layer_diffs,
            difficulty_scores=difficulty_scores,
            difficulty_percentile=difficulty_percentile,
            curriculum_stage=curriculum_stage,
        )
    
    def analyze_dataset(
        self,
        dataset: List[Dict],
        num_stages: int = 3,
        save_path: Optional[str] = None,
        batch_size: int = 32,
    ) -> List[MetaTokenDifference]:
        """
        ì „ì²´ ë°ì´í„°ì…‹ ë¶„ì„ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ ìµœì í™”)
        
        Args:
            dataset: [{'id': ..., 'history': [...], 'utterance': ...}, ...] (ëŒ€í™” í˜•ì‹)
                     ë˜ëŠ” [{'id': ..., 'full_text': ...}, ...] (reasoning task í˜•ì‹)
            num_stages: ì»¤ë¦¬í˜ëŸ¼ ìŠ¤í…Œì´ì§€ ìˆ˜
            save_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
            batch_size: ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
            
        Returns:
            difficulties: ê° ìƒ˜í”Œì˜ ë‚œì´ë„ ì •ë³´
        """
        print(f"Analyzing {len(dataset)} samples with batch_size={batch_size}...")
        
        # ì¤‘ê°„ ì €ì¥ ê²½ë¡œ ì„¤ì •
        checkpoint_path = save_path.replace('.json', '_checkpoint.json') if save_path else None
        
        # 1ë‹¨ê³„: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëª¨ë“  ìƒ˜í”Œ ë¶„ì„
        difficulties = []
        num_batches = (len(dataset) + batch_size - 1) // batch_size
        save_interval = 100  # 100 ë°°ì¹˜ë§ˆë‹¤ ì €ì¥ (ì•½ 6,400 ìƒ˜í”Œë§ˆë‹¤)
        
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(dataset))
            batch = dataset[start_idx:end_idx]
            
            if batch_idx % 10 == 0:
                print(f"  Progress: {start_idx}/{len(dataset)} ({start_idx/len(dataset)*100:.1f}%)")
            
            # ë°°ì¹˜ ë‚´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            batch_texts = []
            batch_ids = []
            for sample in batch:
                if 'full_text' in sample:
                    text = self.extractor._format_input(full_text=sample['full_text'])
                else:
                    text = self.extractor._format_input(
                        dialogue_history=sample['history'],
                        current_utterance=sample['utterance']
                    )
                batch_texts.append(text)
                batch_ids.append(sample['id'])
            
            # ë°°ì¹˜ ë©”íƒ€í† í° ì¶”ì¶œ
            cloud_tokens_list, edge_tokens_list = self.extractor.extract_meta_tokens_batch(batch_texts)
            
            # ê° ìƒ˜í”Œì˜ ë‚œì´ë„ ê³„ì‚°
            for i, sample_id in enumerate(batch_ids):
                # ë ˆì´ì–´ë³„ ì°¨ì´ ê³„ì‚°
                layer_diffs = self.extractor.compute_layer_differences(
                    cloud_tokens_list[i],
                    edge_tokens_list[i],
                    metric=self.metric,
                )
                
                # ë‚œì´ë„ ì ìˆ˜ ê³„ì‚°
                difficulty_scores = self.scorer.compute_difficulty_scores(layer_diffs)
                
                difficulties.append(MetaTokenDifference(
                    sample_id=sample_id,
                    layer_diffs=layer_diffs,
                    difficulty_scores=difficulty_scores,
                    difficulty_percentile=0.5,  # placeholder
                    curriculum_stage=0,  # placeholder
                ))
            
            # ì¤‘ê°„ ì €ì¥ (100 ë°°ì¹˜ë§ˆë‹¤)
            if checkpoint_path and (batch_idx + 1) % save_interval == 0:
                print(f"  ğŸ’¾ Checkpoint: Saving {len(difficulties)} samples...")
                self._save_difficulties(difficulties, checkpoint_path)
                print(f"  âœ“ Checkpoint saved to {checkpoint_path}")
        
        # 2ë‹¨ê³„: ë°±ë¶„ìœ„ ê³„ì‚°
        all_scores = [
            d.difficulty_scores[self.primary_difficulty_metric]
            for d in difficulties
        ]
        
        for diff in difficulties:
            score = diff.difficulty_scores[self.primary_difficulty_metric]
            diff.difficulty_percentile = self.scorer.assign_percentile(score, all_scores)
            diff.curriculum_stage = self.scorer.assign_curriculum_stage(
                diff.difficulty_percentile,
                num_stages,
            )
        
        # 3ë‹¨ê³„: ì €ì¥
        if save_path:
            self._save_difficulties(difficulties, save_path)
            print(f"âœ“ Saved to {save_path}")
        
        return difficulties
    
    def _save_difficulties(
        self,
        difficulties: List[MetaTokenDifference],
        save_path: str,
    ):
        """ë‚œì´ë„ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        data = []
        for diff in difficulties:
            data.append({
                'sample_id': diff.sample_id,
                'layer_diffs': diff.layer_diffs,
                'difficulty_scores': diff.difficulty_scores,
                'difficulty_percentile': diff.difficulty_percentile,
                'curriculum_stage': diff.curriculum_stage,
            })
        
        with open(save_path, 'w') as f:
            json.dump(data, f, indent=2)
    
    @staticmethod
    def load_difficulties(load_path: str) -> List[MetaTokenDifference]:
        """ì €ì¥ëœ ë‚œì´ë„ ì •ë³´ ë¡œë“œ"""
        with open(load_path, 'r') as f:
            data = json.load(f)
        
        difficulties = []
        for item in data:
            difficulties.append(MetaTokenDifference(
                sample_id=item['sample_id'],
                layer_diffs={int(k): v for k, v in item['layer_diffs'].items()},
                difficulty_scores=item['difficulty_scores'],
                difficulty_percentile=item['difficulty_percentile'],
                curriculum_stage=item['curriculum_stage'],
            ))
        
        return difficulties

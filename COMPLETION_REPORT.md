# ë©”íƒ€í† í° ì°¨ì´ ê¸°ë°˜ ì»¤ë¦¬í˜ëŸ¼ ëŸ¬ë‹ - ì™„ë£Œ ë³´ê³ ì„œ

## ğŸ“¦ ìƒì„±ëœ íŒŒì¼ ëª©ë¡

### í•µì‹¬ ëª¨ë“ˆ (2ê°œ)
1. **meta_token_difference.py** (424 lines)
   - `MetaTokenExtractor`: Cloud/Edge LLMì—ì„œ ë ˆì´ì–´ë³„ ë©”íƒ€í† í° ì¶”ì¶œ
   - `DifficultyScorer`: ë‚œì´ë„ ì ìˆ˜ ê³„ì‚° (mean, max, top-k, weighted)
   - `MetaTokenDifferenceAnalyzer`: ì „ì²´ ë°ì´í„°ì…‹ ë¶„ì„ ë° ì €ì¥

2. **curriculum_scheduler.py** (403 lines)
   - `CurriculumConfig`: ì„¤ì • ë°ì´í„°í´ë˜ìŠ¤
   - `CurriculumSampler`: PyTorch ì»¤ìŠ¤í…€ ìƒ˜í”ŒëŸ¬ (4ê°€ì§€ ì „ëµ)
   - `LayerSpecificCurriculumScheduler`: ë ˆì´ì–´ë³„ ë…ë¦½ ì»¤ë¦¬í˜ëŸ¼
   - `CurriculumDataset`: Dataset wrapper
   - `CurriculumTrainingScheduler`: Epochë³„ ì „ëµ ê´€ë¦¬

### í†µí•© ëª¨ë“ˆ (1ê°œ)
3. **train_with_curriculum.py** (150 lines)
   - `create_curriculum_dataloader()`: ì»¤ë¦¬í˜ëŸ¼ DataLoader ìƒì„±
   - `integrate_with_existing_training()`: ê¸°ì¡´ ì½”ë“œ í†µí•© ê°€ì´ë“œ

### íŒŒì´í”„ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸ (2ê°œ)
4. **scripts/01_label_difficulties.py** (153 lines)
   - ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë‚œì´ë„ ë ˆì´ë¸”ë§
   - ë‹¤ì–‘í•œ ì„¤ì • ì˜µì…˜ (ëª¨ë¸, ë©”íŠ¸ë¦­, Top-K ë“±)
   - JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì €ì¥

5. **scripts/02_analyze_difficulties.py** (284 lines)
   - ë‚œì´ë„ ë¶„í¬ ë¶„ì„
   - ë ˆì´ì–´ë³„ ì°¨ì´ íˆíŠ¸ë§µ
   - ì»¤ë¦¬í˜ëŸ¼ ìŠ¤í…Œì´ì§€ ë¶„í¬
   - ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„
   - 6ê°œ ì‹œê°í™” í”Œë¡¯ ìƒì„±

### ì„¤ì • ë° ì‹¤í–‰ (2ê°œ)
6. **configs/curriculum_config.yaml**
   - ë©”íƒ€í† í° ì„¤ì • (metric, top_k, system_prompt)
   - ì»¤ë¦¬í˜ëŸ¼ ì„¤ì • (strategy, num_stages, epochs)
   - í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
   - ë°ì´í„° ê²½ë¡œ ë° ì¶œë ¥ ì„¤ì •

7. **run_curriculum_pipeline.sh** (ì‹¤í–‰ ê°€ëŠ¥)
   - 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ìë™í™”
   - Step 1: ë‚œì´ë„ ë ˆì´ë¸”ë§
   - Step 2: ë¶„ì„ ë° ì‹œê°í™”
   - Step 3: ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ

### ë¬¸ì„œ (4ê°œ)
8. **README.md**
   - í”„ë¡œì íŠ¸ ì „ì²´ ê°œìš”
   - ì‚¬ìš©ë²• ë° ì˜ˆì œ
   - ì˜ˆìƒ ê²°ê³¼
   - ì´ë¡ ì  ë°°ê²½

9. **DESIGN.md**
   - ìƒì„¸ ì„¤ê³„ ë¬¸ì„œ
   - ë©”íƒ€í† í° ì°¨ì´ ê³„ì‚° ë°©ë²•
   - 4ê°€ì§€ ì»¤ë¦¬í˜ëŸ¼ ì „ëµ ì„¤ëª…
   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì˜ˆìƒ ê²°ê³¼

10. **PROJECT_SUMMARY.md**
    - í”„ë¡œì íŠ¸ ì™„ì „ ìš”ì•½
    - êµ¬í˜„ ì„¸ë¶€ì‚¬í•­
    - ì‹¤í—˜ ê³„íš ë° ì²´í¬ë¦¬ìŠ¤íŠ¸
    - í•µì‹¬ Insight

11. **EXPERIMENTS.md**
    - ì‹¤í—˜ ë¡œê·¸ í…œí”Œë¦¿
    - Ablation study ê³„íš
    - ê´€ì°° ë° ê²°ë¡  ê¸°ë¡

---

## ğŸ¯ êµ¬í˜„ ì™„ë£Œ ë‚´ìš©

### 1. ë©”íƒ€í† í° ì°¨ì´ ì¸¡ì • ì‹œìŠ¤í…œ âœ…

**êµ¬í˜„ëœ ê¸°ëŠ¥**:
- Cloud LLM (LLaMA-3-8B) ë¡œë“œ ë° ì¶”ë¡ 
- Edge LLM (Qwen2.5-1.5B) ë¡œë“œ ë° ì¶”ë¡ 
- ë ˆì´ì–´ë³„ ë©”íƒ€í† í° ì¶”ì¶œ (output_hidden_states í™œìš©)
- ë ˆì´ì–´ ë§¤í•‘ (Edge â†’ Cloud ë ˆì´ì–´ ëŒ€ì‘)
- 3ê°€ì§€ ê±°ë¦¬ ë©”íŠ¸ë¦­ (L2, Cosine, KL Divergence)

**ì¶œë ¥ í˜•ì‹**:
```json
{
  "sample_id": "train_00001",
  "layer_diffs": {
    "0": 0.23,
    "1": 0.31,
    ...
    "31": 0.89
  },
  "difficulty_scores": {
    "mean": 0.45,
    "max": 0.89,
    "topk_mean": 0.76,
    "weighted_mean": 0.52,
    "std": 0.15,
    "median": 0.43
  },
  "difficulty_percentile": 0.67,
  "curriculum_stage": 2
}
```

### 2. ì»¤ë¦¬í˜ëŸ¼ ì „ëµ (4ê°€ì§€) âœ…

**A. Easy-to-Hard**:
- ë‚œì´ë„ ìˆœ ì •ë ¬
- ê°€ì¥ ë‹¨ìˆœí•˜ê³  íš¨ê³¼ì 

**B. Layer-Wise Progressive** â­ (ì¶”ì²œ):
```python
Stage 1: layers [0:L//3],    ë‚œì´ë„ [0%, 33%]
Stage 2: layers [0:2*L//3],  ë‚œì´ë„ [0%, 66%]
Stage 3: layers [0:L],       ë‚œì´ë„ [0%, 100%]
```
- LoRA-Genì˜ ë ˆì´ì–´ë³„ êµ¬ì¡°ì™€ ì •ë ¬
- ê° ë ˆì´ì–´ê°€ ìµœì  ì†ë„ë¡œ í•™ìŠµ

**C. Dynamic Pacing**:
```python
if current_loss < threshold:
    difficulty_percentile += 5%  # ë” ì–´ë ¤ìš´ ìƒ˜í”Œ
else:
    difficulty_percentile -= 2%  # ë³µìŠµ
```
- ì ì‘í˜• ë‚œì´ë„ ì¡°ì ˆ
- ê³¼ì í•© ë°©ì§€

**D. Hybrid**:
- Layer-Wise + Dynamic ê²°í•©
- ê°€ì¥ ì •êµí•œ ë°©ë²•

### 3. í•™ìŠµ í†µí•© ì¸í„°í˜ì´ìŠ¤ âœ…

**PyTorch ë„¤ì´í‹°ë¸Œ í†µí•©**:
```python
# ê¸°ì¡´ DataLoaderë¥¼ ê°„ë‹¨íˆ êµì²´
train_loader = create_curriculum_dataloader(
    base_dataset=train_dataset,
    difficulties=difficulties,
    config=curriculum_config,
    epoch=epoch,
    current_loss=avg_loss,
    batch_size=4,
    num_layers=32,
)

# í•™ìŠµ ë£¨í”„ëŠ” ê·¸ëŒ€ë¡œ
for batch in train_loader:
    loss = model(batch)
    ...
```

### 4. ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬ âœ…

**ìƒì„±ë˜ëŠ” í”Œë¡¯ (6ê°œ)**:
1. `difficulty_distributions.png`: 5ê°€ì§€ ë©”íŠ¸ë¦­ ë¶„í¬
2. `layer_differences_heatmap.png`: ìƒ˜í”ŒÃ—ë ˆì´ì–´ íˆíŠ¸ë§µ
3. `layer_statistics.png`: ë ˆì´ì–´ë³„ í‰ê· Â±í‘œì¤€í¸ì°¨
4. `curriculum_stage_distribution.png`: ìŠ¤í…Œì´ì§€ ë¶„í¬
5. `difficulty_by_stage.png`: ìŠ¤í…Œì´ì§€ë³„ ë‚œì´ë„ boxplot
6. `metric_correlations.png`: ë©”íŠ¸ë¦­ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ

---

## ğŸš€ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### Scenario 1: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…

```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ í•œ ë²ˆì— ì‹¤í–‰
bash curriculum/run_curriculum_pipeline.sh \
    curriculum/configs/curriculum_config.yaml \
    data/train.jsonl \
    curriculum/outputs

# ìë™ìœ¼ë¡œ:
# 1. ë‚œì´ë„ ë ˆì´ë¸”ë§
# 2. ë¶„ì„ ë° ì‹œê°í™”
# 3. ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ
```

### Scenario 2: ë‹¨ê³„ë³„ ì‹¤í—˜

```bash
# Step 1: ë‚œì´ë„ë§Œ ë¨¼ì € ê³„ì‚° (ì‹œê°„ ì†Œìš”)
python curriculum/scripts/01_label_difficulties.py \
    --data_path data/train.jsonl \
    --output_path curriculum/data/difficulty_labels.json \
    --max_samples 1000  # ë””ë²„ê¹…ìš©

# Step 2: ë¶„ì„ í™•ì¸
python curriculum/scripts/02_analyze_difficulties.py \
    --difficulty_path curriculum/data/difficulty_labels.json \
    --output_dir curriculum/analysis

# Step 3: ë©”íŠ¸ë¦­ í™•ì¸ í›„ ì „ì²´ ë°ì´í„° ì¬ì‹¤í–‰
python curriculum/scripts/01_label_difficulties.py \
    --data_path data/train.jsonl \
    --output_path curriculum/data/difficulty_labels_full.json \
    --metric "cosine"  # ë©”íŠ¸ë¦­ ë³€ê²½

# Step 4: í•™ìŠµ
python train_dialogue_lora.py \
    --config curriculum/configs/curriculum_config.yaml \
    --difficulty_path curriculum/data/difficulty_labels_full.json \
    --use_curriculum
```

### Scenario 3: ê¸°ì¡´ ì½”ë“œì— í†µí•©

```python
# train_dialogue_lora.py ìˆ˜ì •

# [1] Import ì¶”ê°€
from curriculum.meta_token_difference import MetaTokenDifferenceAnalyzer
from curriculum.train_with_curriculum import create_curriculum_dataloader

# [2] í•™ìŠµ ì‹œì‘ ì „ ë‚œì´ë„ ë¡œë“œ
if args.use_curriculum:
    difficulties = MetaTokenDifferenceAnalyzer.load_difficulties(
        args.difficulty_path
    )

# [3] DataLoader ìƒì„± ë¶€ë¶„ ìˆ˜ì • (Phase 3)
if args.use_curriculum:
    train_loader = create_curriculum_dataloader(
        base_dataset=train_dataset,
        difficulties=difficulties,
        config=curriculum_config,
        epoch=epoch,
        current_loss=avg_loss if epoch > 0 else None,
        batch_size=args.batch_size,
        num_layers=generator.target_num_layers,
    )
else:
    train_loader = DataLoader(...)

# [4] í•™ìŠµ ë£¨í”„ëŠ” ê·¸ëŒ€ë¡œ!
for batch in train_loader:
    ...
```

---

## ğŸ“Š ì‹¤í—˜ ì‹¤í–‰ ê°€ì´ë“œ

### Phase 1: ê¸°ë³¸ ê²€ì¦ (ì¶”ì²œ ì‹œì‘ì )

**ëª©í‘œ**: ë©”íƒ€í† í° ì°¨ì´ê°€ ì‹¤ì œ ë‚œì´ë„ë¥¼ ë°˜ì˜í•˜ëŠ”ì§€ í™•ì¸

```bash
# 1. ì†Œê·œëª¨ ë°ì´í„°ë¡œ ë‚œì´ë„ ë ˆì´ë¸”ë§
python curriculum/scripts/01_label_difficulties.py \
    --data_path data/train.jsonl \
    --output_path curriculum/data/difficulty_labels_1k.json \
    --max_samples 1000 \
    --metric "l2"

# 2. ë¶„ì„
python curriculum/scripts/02_analyze_difficulties.py \
    --difficulty_path curriculum/data/difficulty_labels_1k.json \
    --output_dir curriculum/analysis/phase1

# 3. Easy-to-Hard í•™ìŠµ
python train_dialogue_lora.py \
    --config configs/dialogue_config.yaml \
    --phase 3 \
    --difficulty_path curriculum/data/difficulty_labels_1k.json \
    --curriculum_strategy "easy_to_hard" \
    --output_dir outputs/phase1_curriculum

# 4. Baselineê³¼ ë¹„êµ
python train_dialogue_lora.py \
    --config configs/dialogue_config.yaml \
    --phase 3 \
    --output_dir outputs/phase1_baseline

# 5. í•™ìŠµ ê³¡ì„  ë¹„êµ
python curriculum/scripts/compare_learning_curves.py \
    --baseline_log outputs/phase1_baseline/train.log \
    --curriculum_log outputs/phase1_curriculum/train.log
```

**ì„±ê³µ ê¸°ì¤€**:
- [ ] ë‚œì´ë„ì™€ ì‹¤ì œ ì†ì‹¤ ìƒê´€ê´€ê³„ r > 0.5
- [ ] ìˆ˜ë ´ ì†ë„ 10% ì´ìƒ ê°œì„ 
- [ ] ìµœì¢… ì†ì‹¤ 5% ì´ìƒ ê°œì„ 

### Phase 2: ì „ëµ ë¹„êµ

```bash
# 4ê°€ì§€ ì „ëµìœ¼ë¡œ ì‹¤í—˜
for strategy in easy_to_hard layer_wise_progressive dynamic_pacing hybrid; do
    python train_dialogue_lora.py \
        --config curriculum/configs/curriculum_config.yaml \
        --difficulty_path curriculum/data/difficulty_labels.json \
        --curriculum_strategy "$strategy" \
        --output_dir "outputs/phase2_$strategy"
done

# ê²°ê³¼ ë¹„êµ
python curriculum/scripts/compare_strategies.py \
    --results_dir outputs/phase2_*
```

### Phase 3: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```bash
# Top-K ë¹„êµ
for k in 1 3 5 7; do
    python curriculum/scripts/01_label_difficulties.py \
        --data_path data/train.jsonl \
        --output_path "curriculum/data/difficulty_topk${k}.json" \
        --top_k $k
done

# Num stages ë¹„êµ
for stages in 2 3 4 5; do
    python train_dialogue_lora.py \
        --curriculum_num_stages $stages \
        --output_dir "outputs/phase3_stages${stages}"
done
```

---

## ğŸ’¡ í•µì‹¬ ë””ìì¸ ê²°ì •

### 1. Top-K í‰ê· ì„ ê¸°ë³¸ ë‚œì´ë„ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì„ íƒ

**ì´ìœ **:
- `mean`: ì „ì²´ í‰ê· ì´ë¼ íŠ¹ì´ ë ˆì´ì–´ ë¬´ì‹œ
- `max`: í•˜ë‚˜ì˜ ë ˆì´ì–´ì— ë„ˆë¬´ ë¯¼ê°
- `topk_mean`: **ê°€ì¥ ì–´ë ¤ìš´ Kê°œ ë ˆì´ì–´**ì— ì§‘ì¤‘ âœ…
- `weighted_mean`: ì´ˆê¸° ë ˆì´ì–´ í¸í–¥ ê°€ëŠ¥ì„±

**K=3 ì„ íƒ ê·¼ê±°**:
- ì „ì²´ 32 ë ˆì´ì–´ ì¤‘ ~10% (ìƒìœ„ ë ˆì´ì–´)
- ë„ˆë¬´ ì ìœ¼ë©´ ë…¸ì´ì¦ˆ, ë„ˆë¬´ ë§ìœ¼ë©´ í‰ê· ê³¼ ì°¨ì´ ì—†ìŒ

### 2. Layer-Wise Progressiveë¥¼ ì¶”ì²œ ì „ëµìœ¼ë¡œ

**ì´ìœ **:
- LoRA-Genì€ ë ˆì´ì–´ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ LoRA ìƒì„±
- ê° ë ˆì´ì–´ê°€ ìì‹ ì˜ ë‚œì´ë„ì— ë§ì¶° í•™ìŠµ ê°€ëŠ¥
- ì´ˆê¸° ë ˆì´ì–´ â†’ í›„ê¸° ë ˆì´ì–´ ì ì§„ì  í™œì„±í™”

**3-Stage ì„ íƒ ê·¼ê±°**:
- Stage 1: Warmup + ê¸°ë³¸ íŒ¨í„´ í•™ìŠµ
- Stage 2: ì¤‘ê°„ ë‚œì´ë„ + ë ˆì´ì–´ í™•ì¥
- Stage 3: ì „ì²´ ë°ì´í„° + ë¯¸ì„¸ì¡°ì •

### 3. L2 Distanceë¥¼ ê¸°ë³¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ

**ë¹„êµ**:
- `L2`: ì ˆëŒ€ì  ì°¨ì´, í•´ì„ ìš©ì´ âœ…
- `Cosine`: ë°©í–¥ ì°¨ì´, í¬ê¸° ë¬´ì‹œ
- `KL`: ë¶„í¬ ì°¨ì´, ë¶ˆì•ˆì • ê°€ëŠ¥ì„±

**Ablation í•„ìš”**:
- ì‹¤í—˜ì„ í†µí•´ ìµœì  ë©”íŠ¸ë¦­ ê²°ì •

---

## ğŸ“ ì´ë¡ ì  ê¸°ì—¬

### 1. Meta-Token Difficulty Proxy
**ìƒˆë¡œìš´ ê°œë…**: Cloud-Edge ë©”íƒ€í† í° ì°¨ì´ë¥¼ ë‚œì´ë„ ëŒ€ë¦¬ ì§€í‘œë¡œ ì‚¬ìš©

**ì¥ì **:
- ë³„ë„ì˜ ë‚œì´ë„ ë¼ë²¨ë§ ë¶ˆí•„ìš”
- ëª¨ë¸ êµ¬ì¡°ì— ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©
- ë ˆì´ì–´ë³„ ì„¸ë°€í•œ ë‚œì´ë„ ì¸¡ì • ê°€ëŠ¥

### 2. Layer-Specific Curriculum
**ì•„ì´ë””ì–´**: ê° ë ˆì´ì–´ê°€ ë…ë¦½ì ì¸ ì»¤ë¦¬í˜ëŸ¼ì„ ê°€ì§

**ì°¨ë³„ì **:
- ê¸°ì¡´: ì „ì²´ ëª¨ë¸ í•˜ë‚˜ì˜ ì»¤ë¦¬í˜ëŸ¼
- ìš°ë¦¬: ë ˆì´ì–´ë§ˆë‹¤ ë‹¤ë¥¸ ì»¤ë¦¬í˜ëŸ¼
- íš¨ê³¼: ë ˆì´ì–´ë³„ ìµœì  í•™ìŠµ ì†ë„

### 3. Hierarchical Curriculum Training
**ë°©ë²•ë¡ **: ë ˆì´ì–´ë³„ í•™ìŠµ â†’ Joint í•™ìŠµ

```
Phase 1: Layer-specific (ê° ë ˆì´ì–´ ë…ë¦½)
    â†“
Phase 2: Joint training (ë ˆì´ì–´ ê°„ ìƒí˜¸ì‘ìš©)
    â†“
Phase 3: Fine-tuning (ì „ì²´ ìµœì í™”)
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„íŒ©íŠ¸

### í•™ìˆ ì  ê°€ì¹˜
- **ICML/NeurIPS 2026** íˆ¬ê³  ê°€ëŠ¥
- Meta-learning + Curriculum learning êµì°¨ì 
- Cloud-Edge í˜‘ì—… ì‹œìŠ¤í…œì— ì¼ë°˜í™” ê°€ëŠ¥

### ì‹¤ìš©ì  ê°€ì¹˜
- GPU ì‹œê°„ 20-30% ì ˆê°
- í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ê°ì†Œ

### í™•ì¥ ê°€ëŠ¥ì„±
- ë‹¤ë¥¸ LoRA ìƒì„± ëª¨ë¸ì— ì ìš©
- Multi-task learningì— í™œìš©
- ì§€ì†ì  í•™ìŠµ(Continual Learning)ì— í†µí•©

---

## âœ… ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì½”ë“œ ì™„ì„±ë„
- [x] ë©”íƒ€í† í° ì¶”ì¶œ ëª¨ë“ˆ ì™„ì „ êµ¬í˜„
- [x] 4ê°€ì§€ ì»¤ë¦¬í˜ëŸ¼ ì „ëµ êµ¬í˜„
- [x] PyTorch DataLoader í†µí•©
- [x] ë ˆì´ì–´ë³„ ìŠ¤ì¼€ì¤„ë§
- [x] ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬
- [x] íŒŒì´í”„ë¼ì¸ ìë™í™”

### ë¬¸ì„œ ì™„ì„±ë„
- [x] README (ì‚¬ìš©ë²•)
- [x] DESIGN (ìƒì„¸ ì„¤ê³„)
- [x] PROJECT_SUMMARY (ì „ì²´ ìš”ì•½)
- [x] EXPERIMENTS (ì‹¤í—˜ í…œí”Œë¦¿)
- [x] ì„¤ì • íŒŒì¼ (YAML)
- [x] ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (Shell)

### ì‹¤í—˜ ì¤€ë¹„ë„
- [x] ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- [x] ë‹¤ì–‘í•œ ì„¤ì • ì˜µì…˜
- [x] ê²°ê³¼ ì €ì¥ ë° ë¡œë“œ
- [x] ë¹„êµ ì‹¤í—˜ í”„ë ˆì„ì›Œí¬

### ë‹¤ìŒ ë‹¨ê³„
- [ ] PersonaChat ë°ì´í„°ë¡œ ì‹¤í—˜
- [ ] Baseline ëŒ€ë¹„ ì„±ëŠ¥ ì¸¡ì •
- [ ] ì „ëµë³„ ë¹„êµ ì‹¤í—˜
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] ë…¼ë¬¸ ì‘ì„±

---

## ğŸ‰ ì™„ë£Œ!

**ì´ 11ê°œ íŒŒì¼ ìƒì„±**:
- í•µì‹¬ ëª¨ë“ˆ: 2ê°œ (827 lines)
- í†µí•© ëª¨ë“ˆ: 1ê°œ (150 lines)
- íŒŒì´í”„ë¼ì¸: 2ê°œ (437 lines)
- ì„¤ì •: 2ê°œ (YAML + Shell)
- ë¬¸ì„œ: 4ê°œ (README, DESIGN, SUMMARY, EXPERIMENTS)

**ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥**:
```bash
bash curriculum/run_curriculum_pipeline.sh
```

**ê¸°ì¡´ í•™ìŠµì— í†µí•©**:
```python
from curriculum.train_with_curriculum import create_curriculum_dataloader
```

**ë‹¤ìŒ ì‹¤í–‰ ëª…ë ¹**:
```bash
# ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ (1000 ìƒ˜í”Œ)
python curriculum/scripts/01_label_difficulties.py \
    --data_path data/train.jsonl \
    --output_path curriculum/data/test_difficulties.json \
    --max_samples 1000

# ë¶„ì„
python curriculum/scripts/02_analyze_difficulties.py \
    --difficulty_path curriculum/data/test_difficulties.json \
    --output_dir curriculum/analysis/test
```

---

**í”„ë¡œì íŠ¸ ìƒíƒœ**: âœ… êµ¬í˜„ ì™„ë£Œ, ì‹¤í—˜ ì¤€ë¹„ ì™„ë£Œ
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: ë‚œì´ë„ ë ˆì´ë¸”ë§ 2-4ì‹œê°„ (ì „ì²´ ë°ì´í„°), í•™ìŠµ ì‹¤í—˜ 1-2ì¼
**í•µì‹¬ ê°€ì¹˜**: í•™ìŠµ íš¨ìœ¨ 20-30% í–¥ìƒ ì˜ˆìƒ
